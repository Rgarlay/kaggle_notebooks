{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppresses TensorFlow INFO and WARNING logs\nos.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n# Optional: also suppress absl warnings\nimport absl.logging\nabsl.logging.set_verbosity(absl.logging.ERROR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:19.082710Z","iopub.execute_input":"2025-08-05T13:13:19.083332Z","iopub.status.idle":"2025-08-05T13:13:19.087614Z","shell.execute_reply.started":"2025-08-05T13:13:19.083306Z","shell.execute_reply":"2025-08-05T13:13:19.086726Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth',None)\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense, Embedding, Concatenate, Input, LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.layers import SimpleRNN, Dense,Reshape, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Attention, Flatten, GlobalAveragePooling1D,BatchNormalization, Bidirectional, Add, GRU\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:20.310433Z","iopub.execute_input":"2025-08-05T13:13:20.310995Z","iopub.status.idle":"2025-08-05T13:13:20.317734Z","shell.execute_reply.started":"2025-08-05T13:13:20.310974Z","shell.execute_reply":"2025-08-05T13:13:20.316950Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n/kaggle/input/jigsaw-agile-community-rules/train.csv\n/kaggle/input/jigsaw-agile-community-rules/test.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"combined_stopwords = {\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n \"'ve\",\n 'a',\n 'about',\n 'above',\n 'across',\n 'after',\n 'afterwards',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'almost',\n 'alone',\n 'along',\n 'already',\n 'also',\n 'although',\n 'always',\n 'am',\n 'among',\n 'amongst',\n 'amount',\n 'an',\n 'and',\n 'another',\n 'any',\n 'anyhow',\n 'anyone',\n 'anything',\n 'anyway',\n 'anywhere',\n 'are',\n 'aren',\n \"aren't\",\n 'around',\n 'as',\n 'at',\n 'back',\n 'be',\n 'became',\n 'because',\n 'become',\n 'becomes',\n 'becoming',\n 'been',\n 'before',\n 'beforehand',\n 'behind',\n 'being',\n 'below',\n 'beside',\n 'besides',\n 'between',\n 'beyond',\n 'both',\n 'bottom',\n 'but',\n 'by',\n 'ca',\n 'call',\n 'can',\n 'cannot',\n 'could',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'done',\n 'down',\n 'due',\n 'during',\n 'each',\n 'eight',\n 'either',\n 'eleven',\n 'else',\n 'elsewhere',\n 'empty',\n 'enough',\n 'even',\n 'ever',\n 'every',\n 'everyone',\n 'everything',\n 'everywhere',\n 'except',\n 'few',\n 'fifteen',\n 'fifty',\n 'first',\n 'five',\n 'for',\n 'former',\n 'formerly',\n 'forty',\n 'four',\n 'from',\n 'front',\n 'full',\n 'further',\n 'get',\n 'give',\n 'go',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n \"he'd\",\n \"he'll\",\n \"he's\",\n 'hence',\n 'her',\n 'here',\n 'hereafter',\n 'hereby',\n 'herein',\n 'hereupon',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'however',\n 'hundred',\n 'i',\n \"i'd\",\n \"i'll\",\n \"i'm\",\n \"i've\",\n 'if',\n 'in',\n 'indeed',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it'd\",\n \"it'll\",\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'keep',\n 'last',\n 'latter',\n 'latterly',\n 'least',\n 'less',\n 'll',\n 'm',\n 'ma',\n 'made',\n 'make',\n 'many',\n 'may',\n 'me',\n 'meanwhile',\n 'might',\n 'mightn',\n \"mightn't\",\n 'mine',\n 'more',\n 'moreover',\n 'most',\n 'mostly',\n 'move',\n 'much',\n 'must',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n \"n't\",\n 'name',\n 'namely',\n 'needn',\n \"needn't\",\n 'neither',\n 'never',\n 'nevertheless',\n 'next',\n 'nine',\n 'no',\n 'nobody',\n 'none',\n 'noone',\n 'nor',\n 'not',\n 'nothing',\n 'now',\n 'nowhere',\n 'n‘t',\n 'n’t',\n 'o',\n 'of',\n 'off',\n 'often',\n 'on',\n 'once',\n 'one',\n 'only',\n 'onto',\n 'or',\n 'other',\n 'others',\n 'otherwise',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 'part',\n 'per',\n 'perhaps',\n 'please',\n 'put',\n 'quite',\n 'rather',\n 're',\n 'really',\n 'regarding',\n 's',\n 'same',\n 'say',\n 'see',\n 'seem',\n 'seemed',\n 'seeming',\n 'seems',\n 'serious',\n 'several',\n 'shan',\n \"shan't\",\n 'she',\n \"she'd\",\n \"she'll\",\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'show',\n 'side',\n 'since',\n 'six',\n 'sixty',\n 'so',\n 'some',\n 'somehow',\n 'someone',\n 'something',\n 'sometime',\n 'sometimes',\n 'somewhere',\n 'still',\n 'such',\n 't',\n 'take',\n 'ten',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'thence',\n 'there',\n 'thereafter',\n 'thereby',\n 'therefore',\n 'therein',\n 'thereupon',\n 'these',\n 'they',\n \"they'd\",\n \"they'll\",\n \"they're\",\n \"they've\",\n 'third',\n 'this',\n 'those',\n 'though',\n 'three',\n 'through',\n 'throughout',\n 'thru',\n 'thus',\n 'to',\n 'together',\n 'too',\n 'top',\n 'toward',\n 'towards',\n 'twelve',\n 'twenty',\n 'two',\n 'under',\n 'unless',\n 'until',\n 'up',\n 'upon',\n 'us',\n 'used',\n 'using',\n 'various',\n 've',\n 'very',\n 'via',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n \"we'd\",\n \"we'll\",\n \"we're\",\n \"we've\",\n 'well',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'whatever',\n 'when',\n 'whence',\n 'whenever',\n 'where',\n 'whereafter',\n 'whereas',\n 'whereby',\n 'wherein',\n 'whereupon',\n 'wherever',\n 'whether',\n 'which',\n 'while',\n 'whither',\n 'who',\n 'whoever',\n 'whole',\n 'whom',\n 'whose',\n 'why',\n 'will',\n 'with',\n 'within',\n 'without',\n 'won',\n \"won't\",\n 'would',\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'yet',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n '‘d',\n '‘ll',\n '‘m',\n '‘re',\n '‘s',\n '‘ve',\n '’d',\n '’ll',\n '’m',\n '’re',\n '’s',\n '’ve'}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:23.580019Z","iopub.execute_input":"2025-08-05T13:13:23.580340Z","iopub.status.idle":"2025-08-05T13:13:23.595202Z","shell.execute_reply.started":"2025-08-05T13:13:23.580317Z","shell.execute_reply":"2025-08-05T13:13:23.594459Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"df_train_original = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\ndf_test_original = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:23.819623Z","iopub.execute_input":"2025-08-05T13:13:23.820150Z","iopub.status.idle":"2025-08-05T13:13:23.853741Z","shell.execute_reply.started":"2025-08-05T13:13:23.820126Z","shell.execute_reply":"2025-08-05T13:13:23.853218Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# def dataset_basics(df):\n#     print(f'Number of empty rows \\n {df_train.isna().sum()} \\n')\n#     print(f'Number of duplicate rows {df.duplicated().sum()} \\n')\n#     print(f'\\n Dtypes and other info \\n {df.info()} \\n')\n#     print(f'Shape of the dataframe {df.shape}')\n#     print(f'no_of unique subreddits',df['subreddit'].nunique())\n#     print(f'Percent of 0s and 1s: ',df['rule_violation'].value_counts()/df_train.shape[0] * 100)\n# dataset_basics(df_train_original)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:25.523842Z","iopub.execute_input":"2025-08-05T13:13:25.524368Z","iopub.status.idle":"2025-08-05T13:13:25.527655Z","shell.execute_reply.started":"2025-08-05T13:13:25.524343Z","shell.execute_reply":"2025-08-05T13:13:25.526926Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"df_train = df_train_original['rule'].str.split(',',expand=True)\n\ndf_train.fillna('0',inplace=True)\n\ndf_train.columns = ['rule 1','rule 2','rule 3','rule 4']\n\n## till here it works perfectly\n\ndf_train['rule 2'] = df_train['rule 2'].apply(lambda x : 1 if 'referral links' in str(x).lower() else 0)\n\ndf_train = df_train.rename(columns = {'rule 2':'referral link allowed'})\n\n#column 0 (legal_advice)\n\ndf_train['legal_advice_allowed'] = df_train['rule 1'].apply(lambda x: 0 if 'legal' in str(x).lower() else 1)\n\n#column 1\ndf_train['rule 4'] = df_train['rule 4'].apply(lambda x: 0 if 'promotional' in str(x).lower() else 1)\n\ndf_train = df_train.rename(columns = {'rule 4':'Promotions allowed'})\n\n##column 1: advertising\n\ndf_train['rule 3'] = df_train['rule 3'].apply(lambda x: 0 if 'advertising' in str(x).lower() else 1)\n\ndf_train = df_train.rename(columns = {'rule 3':'Advertising allowed'})\n\n## Dealing with  spam part using new created dataframe\n\ndf_train_1 = df_train['rule 1'].str.split(':',expand=True)\n\ndf_train = df_train.drop(columns = ['rule 1'])\n\ndf_train_1.columns = ['Advert','spam']\n\ndf_train_1 = df_train_1.drop(columns = ['Advert'])\n    \ndf_train_1['spam allowed'] = df_train_1['spam'].apply(lambda x: 0 if 'spam' in str(x).lower() else 1)\n\ndf_train['spam allowed'] = df_train_1['spam allowed']\n\n### Testing Data \n\ndf_test = df_test_original['rule'].str.split(',', expand=True)\n\ndf_test.fillna('0', inplace=True)\n\ndf_test.columns = ['rule 1', 'rule 2', 'rule 3', 'rule 4']\n\n## till here it works perfectly\n\ndf_test['rule 2'] = df_test['rule 2'].apply(lambda x: 1 if 'referral links' in str(x).lower() else 0)\n\ndf_test = df_test.rename(columns={'rule 2': 'referral link allowed'})\n\n# column 0 (legal_advice)\ndf_test['legal_advice_allowed'] = df_test['rule 1'].apply(lambda x: 0 if 'legal' in str(x).lower() else 1)\n\n# column 1\ndf_test['rule 4'] = df_test['rule 4'].apply(lambda x: 0 if 'promotional' in str(x).lower() else 1)\n\ndf_test = df_test.rename(columns={'rule 4': 'Promotions allowed'})\n\n# column 1: advertising\ndf_test['rule 3'] = df_test['rule 3'].apply(lambda x: 0 if 'advertising' in str(x).lower() else 1)\n\ndf_test = df_test.rename(columns={'rule 3': 'Advertising allowed'})\n\n## Dealing with spam part using new created dataframe\ndf_test_1 = df_test['rule 1'].str.split(':', expand=True)\n\ndf_test = df_test.drop(columns=['rule 1'])\n\ndf_test_1.columns = ['Advert', 'spam']\n\ndf_test_1 = df_test_1.drop(columns=['Advert'])\n    \ndf_test_1['spam allowed'] = df_test_1['spam'].apply(lambda x: 0 if 'spam' in str(x).lower() else 1)\n\ndf_test['spam allowed'] = df_test_1['spam allowed']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:25.983922Z","iopub.execute_input":"2025-08-05T13:13:25.984190Z","iopub.status.idle":"2025-08-05T13:13:26.017288Z","shell.execute_reply.started":"2025-08-05T13:13:25.984169Z","shell.execute_reply":"2025-08-05T13:13:26.016711Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"for i in df_train.columns:\n    df_train_original[i] = df_train[i]\n\nfor i in df_test.columns:\n    df_test_original[i] = df_test[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:29.297102Z","iopub.execute_input":"2025-08-05T13:13:29.297404Z","iopub.status.idle":"2025-08-05T13:13:29.305168Z","shell.execute_reply.started":"2025-08-05T13:13:29.297379Z","shell.execute_reply":"2025-08-05T13:13:29.304505Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"df_original = df_train_original[['body', 'rule_violation','referral link allowed', 'Advertising allowed',\n           'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]\n    \ndf_p1 = df_train_original[['positive_example_1','rule_violation','referral link allowed', 'Advertising allowed',\n               'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]\n\ndf_p1.loc[:,'rule_violation'] = 0\ndf_p1 = df_p1.rename(columns = {'positive_example_1':'body'})\n        \ndf_p2 = df_train_original[['positive_example_2','rule_violation','referral link allowed', 'Advertising allowed',\n               'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]\n\ndf_p2.loc[:,'rule_violation'] = 0\ndf_p2 = df_p2.rename(columns = {'positive_example_2':'body'})\n        \ndf_n1 = df_train_original[['negative_example_1','rule_violation','referral link allowed', 'Advertising allowed',\n               'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]\n\ndf_n1.loc[:,'rule_violation'] = 1\ndf_n1 = df_n1.rename(columns = {'negative_example_1':'body'})\n        \ndf_n2 = df_train_original[['negative_example_2','rule_violation','referral link allowed', 'Advertising allowed',\n               'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]\n\ndf_n2.loc[:,'rule_violation'] = 1\ndf_n2 = df_n2.rename(columns = {'negative_example_2':'body'})\n    \ndf_train_total = pd.concat([df_original,df_p1,df_p2,df_n1,df_n2], axis = 0,ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:29.593113Z","iopub.execute_input":"2025-08-05T13:13:29.593589Z","iopub.status.idle":"2025-08-05T13:13:29.611008Z","shell.execute_reply.started":"2025-08-05T13:13:29.593565Z","shell.execute_reply":"2025-08-05T13:13:29.610385Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"text_file = df_train_total['body'].to_list()\n    \npattern = re.compile('https?:\\/\\/\\S+|www\\.\\S+|Https?:\\/\\/\\S+|\\S+\\.com\\S+|\\S+\\.com|\\[.*?\\]|\\S+ \\. com.*')   ## Removing URLs\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i])\n    \npattern = re.compile('<.*?>')       ##Removing HTML rags\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i]) \n    \npattern = re.compile('#\\S+|@\\S+|\\S+\\@\\S+|\\S+@')             ## Removing Emails and Hashtags\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i])            ### Removing username and subreddit mentions\n    \npattern = re.compile('u\\/\\S+|r\\/\\S+')\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i])\n        \npattern = re.compile(\"[\"\n                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                        u\"\\U00002702-\\U000027B0\"\n                        u\"\\U000024C2-\\U0001F251\"\n                        \"]+\", flags=re.UNICODE)\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i])\n\npattern = re.compile('\\d|\\\\n')             ##Removing Numbers & \\n spaces\nfor i in range(len(text_file)):\n    text_file[i] = pattern.sub(r'',text_file[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:33.624730Z","iopub.execute_input":"2025-08-05T13:13:33.624972Z","iopub.status.idle":"2025-08-05T13:13:34.040404Z","shell.execute_reply.started":"2025-08-05T13:13:33.624954Z","shell.execute_reply":"2025-08-05T13:13:34.039796Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"## Works perfectly\nx_feature_train = df_train_total.drop(columns = ['rule_violation'])\ny_target = df_train_total['rule_violation']\n\nx_feature_train = x_feature_train.drop(columns = ['body'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:38.064494Z","iopub.execute_input":"2025-08-05T13:13:38.064738Z","iopub.status.idle":"2025-08-05T13:13:38.070919Z","shell.execute_reply.started":"2025-08-05T13:13:38.064721Z","shell.execute_reply":"2025-08-05T13:13:38.070371Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ncleaned_text = []\npreprocessed_training_text = []\nfor i in range(len(x_feature_train)):\n    text1 = re.sub('[^a-zA-Z]',' ',text_file[i])\n    text1 = text1.lower()\n    text1 = [lemmatizer.lemmatize(word) for word in text1 if word not in combined_stopwords]\n    cleaned_text = ''.join(text1)\n    preprocessed_training_text.append(cleaned_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:43.484725Z","iopub.execute_input":"2025-08-05T13:13:43.485482Z","iopub.status.idle":"2025-08-05T13:13:46.134056Z","shell.execute_reply.started":"2025-08-05T13:13:43.485453Z","shell.execute_reply":"2025-08-05T13:13:46.133286Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"## we have x,y, preprocessed_training_text.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:46.135272Z","iopub.execute_input":"2025-08-05T13:13:46.135506Z","iopub.status.idle":"2025-08-05T13:13:46.138838Z","shell.execute_reply.started":"2025-08-05T13:13:46.135490Z","shell.execute_reply":"2025-08-05T13:13:46.138040Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"df_test_total = df_test_original[['body','referral link allowed', 'Advertising allowed',\n           'Promotions allowed', 'legal_advice_allowed', 'spam allowed']]        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:46.139687Z","iopub.execute_input":"2025-08-05T13:13:46.140162Z","iopub.status.idle":"2025-08-05T13:13:46.159408Z","shell.execute_reply.started":"2025-08-05T13:13:46.140144Z","shell.execute_reply":"2025-08-05T13:13:46.158689Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"text_file_1 = df_test_total['body'].to_list()\n    \npattern = re.compile('https?:\\/\\/\\S+|www\\.\\S+|Https?:\\/\\/\\S+|\\S+\\.com\\S+|\\S+\\.com|\\[.*?\\]|\\S+ \\. com.*')   ## Removing URLs\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i])\n    \npattern = re.compile('<.*?>')       ##Removing HTML rags\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i]) \n    \npattern = re.compile('#\\S+|@\\S+|\\S+\\@\\S+|\\S+@')             ## Removing Emails and Hashtags\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i])            ### Removing username and subreddit mentions\n    \npattern = re.compile('u\\/\\S+|r\\/\\S+')\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i])\n        \npattern = re.compile(\"[\"\n                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                        u\"\\U00002702-\\U000027B0\"\n                        u\"\\U000024C2-\\U0001F251\"\n                        \"]+\", flags=re.UNICODE)\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i])\n\npattern = re.compile('\\d|\\\\n')             ##Removing Numbers & \\n spaces\nfor i in range(len(text_file_1)):\n    text_file_1[i] = pattern.sub(r'',text_file_1[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:46.160413Z","iopub.execute_input":"2025-08-05T13:13:46.160635Z","iopub.status.idle":"2025-08-05T13:13:46.178035Z","shell.execute_reply.started":"2025-08-05T13:13:46.160620Z","shell.execute_reply":"2025-08-05T13:13:46.177365Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ncleaned_text = []\npreprocessed_testing_text = []\nfor i in range(len(df_test_total)):\n    text1 = re.sub('[^a-zA-Z]',' ',text_file_1[i])\n    text1 = text1.lower()\n    text1 = [lemmatizer.lemmatize(word) for word in text1 if word not in combined_stopwords]\n    cleaned_text = ''.join(text1)\n    preprocessed_testing_text.append(cleaned_text)\n    \n    ##Works perfectly fine till now.\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:48.008541Z","iopub.execute_input":"2025-08-05T13:13:48.008787Z","iopub.status.idle":"2025-08-05T13:13:48.014982Z","shell.execute_reply.started":"2025-08-05T13:13:48.008769Z","shell.execute_reply":"2025-08-05T13:13:48.014304Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"token = Tokenizer(num_words = 3000, oov_token='<OOV>')\n##training text tokenization and padding\n\ntoken.fit_on_texts(preprocessed_training_text)\nmax_length = max(len(i.split()) for i in preprocessed_training_text)\ntraining_sequence = token.texts_to_sequences(preprocessed_training_text)\nx_train = pad_sequences(training_sequence, maxlen=max_length, padding='post')\n    \n\n\n    # TENSORFLOW ARCHITECTURE HERE\n    ## Feature dataset input preprocessing\nfeature_input = Input(shape=(5,), name = 'other_feature')\ny = Dense(64, activation='relu', kernel_regularizer=l2(0.5))(feature_input)\ny = BatchNormalization()(y)\n\n## Text input preprocessing\ntext_input = Input(shape=(83,), name='text_input')\nembedding = Embedding(input_dim = 3000, output_dim = 128)(text_input)\nx1 = GRU(64, return_sequences=True)(embedding)\nx1 = Dropout(0.5)(x1)\nx1 = Dense(32, activation = 'relu', kernel_regularizer=l2(0.5))(x1)\n\n# Multi-Head Attention for better context\nx2 = Attention()([x1, x1])\n\n# Combine Attention output with LSTM via Residual Connection\n# Pooling to flatten sequence\nx = GlobalAveragePooling1D()(x2)\n\n\ncombined = Concatenate()([x,y])\nrnn1 = Dense(64, activation='relu', kernel_regularizer=l2(0.5))(combined)\ndropout2 = Dropout(0.6)(rnn1)\noutput = Dense(1, activation='sigmoid')(dropout2)\n\nmodel = Model(inputs=[text_input, feature_input], outputs=output)\nadam = Adam(learning_rate = 0.001)\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=[AUC(curve='ROC', name='roc_auc')])\n## This architecture achieved loss: 0.4561 - roc_auc: 0.8632 - val_loss: 0.5426 - val_roc_auc: 0.8094 after 20 iterations. \n\nhistory = model.fit(\n    {'text_input': x_train, 'other_feature': x_feature_train},\n    y_target,\n    epochs=25, batch_size=50, verbose = 1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:13:49.557746Z","iopub.execute_input":"2025-08-05T13:13:49.558011Z","iopub.status.idle":"2025-08-05T13:14:42.418173Z","shell.execute_reply.started":"2025-08-05T13:13:49.557989Z","shell.execute_reply":"2025-08-05T13:14:42.417467Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 36.5198 - roc_auc: 0.4910\nEpoch 2/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.9190 - roc_auc: 0.5012\nEpoch 3/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.7199 - roc_auc: 0.5111\nEpoch 4/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6936 - roc_auc: 0.5017\nEpoch 5/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6931 - roc_auc: 0.5001\nEpoch 6/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4968\nEpoch 7/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6933 - roc_auc: 0.4899\nEpoch 8/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4930\nEpoch 9/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.5016\nEpoch 10/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6932 - roc_auc: 0.4929\nEpoch 11/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6931 - roc_auc: 0.4970\nEpoch 12/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6932 - roc_auc: 0.4917\nEpoch 13/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6932 - roc_auc: 0.5009\nEpoch 14/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4929\nEpoch 15/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4889\nEpoch 16/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4985\nEpoch 17/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4933\nEpoch 18/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6932 - roc_auc: 0.5000\nEpoch 19/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6931 - roc_auc: 0.5042\nEpoch 20/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4968\nEpoch 21/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6932 - roc_auc: 0.4961\nEpoch 22/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6931 - roc_auc: 0.4945\nEpoch 23/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.6931 - roc_auc: 0.4985\nEpoch 24/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6931 - roc_auc: 0.4904\nEpoch 25/25\n\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.6931 - roc_auc: 0.4977\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"### ROC_AUC score for training dataset is 0.88.","metadata":{}},{"cell_type":"code","source":"x_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:22.997320Z","iopub.execute_input":"2025-08-05T13:06:22.997986Z","iopub.status.idle":"2025-08-05T13:06:23.004264Z","shell.execute_reply.started":"2025-08-05T13:06:22.997957Z","shell.execute_reply":"2025-08-05T13:06:23.003557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test_features = df_test_total.drop(columns = ['body'])\ntesting_sequence = token.texts_to_sequences(preprocessed_testing_text)\nx_test = pad_sequences(testing_sequence, maxlen=max_length, padding='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:26.116887Z","iopub.execute_input":"2025-08-05T13:06:26.117166Z","iopub.status.idle":"2025-08-05T13:06:26.122659Z","shell.execute_reply.started":"2025-08-05T13:06:26.117145Z","shell.execute_reply":"2025-08-05T13:06:26.122045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.shape,x_test_features.shape,x_feature_train.shape,x_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:30.041443Z","iopub.execute_input":"2025-08-05T13:06:30.041894Z","iopub.status.idle":"2025-08-05T13:06:30.046859Z","shell.execute_reply.started":"2025-08-05T13:06:30.041871Z","shell.execute_reply":"2025-08-05T13:06:30.046298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict({'text_input': x_test, 'other_feature': x_test_features})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:30.372943Z","iopub.execute_input":"2025-08-05T13:06:30.373354Z","iopub.status.idle":"2025-08-05T13:06:30.638480Z","shell.execute_reply.started":"2025-08-05T13:06:30.373333Z","shell.execute_reply":"2025-08-05T13:06:30.637937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\"row_id\",\"rule_violation\"})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission['rule_violation'] = np.round(y_pred,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:33.409009Z","iopub.execute_input":"2025-08-05T13:06:33.409726Z","iopub.status.idle":"2025-08-05T13:06:33.413462Z","shell.execute_reply.started":"2025-08-05T13:06:33.409701Z","shell.execute_reply":"2025-08-05T13:06:33.412704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:33.954461Z","iopub.execute_input":"2025-08-05T13:06:33.955691Z","iopub.status.idle":"2025-08-05T13:06:33.984904Z","shell.execute_reply.started":"2025-08-05T13:06:33.955646Z","shell.execute_reply":"2025-08-05T13:06:33.983575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"rule_violation\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:06:35.748824Z","iopub.execute_input":"2025-08-05T13:06:35.749418Z","iopub.status.idle":"2025-08-05T13:06:35.759096Z","shell.execute_reply.started":"2025-08-05T13:06:35.749394Z","shell.execute_reply":"2025-08-05T13:06:35.758528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth',None)\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBClassifier\n\n\nfrom tensorflow.keras.layers import Dense, Embedding, Concatenate, Input, LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.layers import SimpleRNN, Dense,Reshape, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Attention, Flatten, GlobalAveragePooling1D,BatchNormalization, Bidirectional, Add, GRU\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:26:50.149110Z","iopub.execute_input":"2025-09-29T10:26:50.149361Z","iopub.status.idle":"2025-09-29T10:27:08.226662Z","shell.execute_reply.started":"2025-09-29T10:26:50.149336Z","shell.execute_reply":"2025-09-29T10:27:08.226018Z"}},"outputs":[{"name":"stderr","text":"2025-09-29 10:26:55.959476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759141616.182457      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759141616.245405      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n/kaggle/input/jigsaw-agile-community-rules/train.csv\n/kaggle/input/jigsaw-agile-community-rules/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"combined_stopwords = {\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards',\n                      'again', 'against', 'ain', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always',\n                      'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', \n                      'anywhere', 'are', 'aren', \"aren't\", 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes',\n                      'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', \n                      'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\",\n                      'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'done', 'down', 'due', 'during', 'each', 'eight', 'either', \n                      'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except',\n                      'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further',\n                      'get', 'give', 'go', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \n                      \"he'll\", \"he's\", 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself',\n                      'his', 'how', 'however', 'hundred', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'indeed', 'into', 'is', 'isn', \"isn't\",\n                      'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'll', 'm', \n                      'ma', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mightn', \"mightn't\", 'mine', 'more', 'moreover', 'most',\n                      'mostly', 'move', 'much', 'must', 'mustn', \"mustn't\", 'my', 'myself', \"n't\", 'name', 'namely', 'needn', \"needn't\", 'neither', \n                      'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t',\n                      'n’t', 'o', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours',\n                      'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding',\n                      's', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'shan', \"shan't\", 'she', \"she'd\",\n                      \"she'll\", \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'show', 'side', 'since', 'six', 'sixty', 'so', 'some',\n                      'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 't', 'take', 'ten', 'than', \n                      'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', \n                      'therefore', 'therein', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'third', 'this', 'those', \n                      'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve',\n                      'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 've', 'very', 'via', 'was', \n                      'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'well', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when',\n                      'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which',\n                      'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', \"won't\",\n                      'would', 'wouldn', \"wouldn't\", 'y', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n                      'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve'}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.230678Z","iopub.execute_input":"2025-09-29T10:27:08.230952Z","iopub.status.idle":"2025-09-29T10:27:08.242087Z","shell.execute_reply.started":"2025-09-29T10:27:08.230935Z","shell.execute_reply":"2025-09-29T10:27:08.241453Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_train_original = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\ndf_test_original = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.242845Z","iopub.execute_input":"2025-09-29T10:27:08.243105Z","iopub.status.idle":"2025-09-29T10:27:08.347537Z","shell.execute_reply.started":"2025-09-29T10:27:08.243083Z","shell.execute_reply":"2025-09-29T10:27:08.346945Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# def dataset_basics(df):\n#     print(f'Number of empty rows \\n {df_train.isna().sum()} \\n')\n#     print(f'Number of duplicate rows {df.duplicated().sum()} \\n')\n#     print(f'\\n Dtypes and other info \\n {df.info()} \\n')\n#     print(f'Shape of the dataframe {df.shape}')\n#     print(f'no_of unique subreddits',df['subreddit'].nunique())\n#     print(f'Percent of 0s and 1s: ',df['rule_violation'].value_counts()/df_train.shape[0] * 100)\n# dataset_basics(df_train_original)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.348302Z","iopub.execute_input":"2025-09-29T10:27:08.348659Z","iopub.status.idle":"2025-09-29T10:27:08.352117Z","shell.execute_reply.started":"2025-09-29T10:27:08.348636Z","shell.execute_reply":"2025-09-29T10:27:08.351432Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# df_train = df_train_original['rule'].str.split(',',expand=True)\n\n# df_train.fillna('0',inplace=True)\n\n#df_train.columns = ['rule 1','rule 2','rule 3','rule 4']\n\n## till here it works perfectly\n\n# df_train['rule 2'] = df_train['rule 2'].apply(lambda x : 1 if 'referral links' in str(x).lower() else 0)\n\n# df_train = df_train.rename(columns = {'rule 2':'referral link allowed'})\n\n# #column 0 (legal_advice)\n\n# df_train['legal_advice_allowed'] = df_train['rule 1'].apply(lambda x: 0 if 'legal' in str(x).lower() else 1)\n\n# #column 1\n# df_train['rule 4'] = df_train['rule 4'].apply(lambda x: 0 if 'promotional' in str(x).lower() else 1)\n\n# df_train = df_train.rename(columns = {'rule 4':'Promotions allowed'})\n\n# ##column 1: advertising\n\n# df_train['rule 3'] = df_train['rule 3'].apply(lambda x: 0 if 'advertising' in str(x).lower() else 1)\n\n# df_train = df_train.rename(columns = {'rule 3':'Advertising allowed'})\n\n# ## Dealing with  spam part using new created dataframe\n\n# df_train_1 = df_train['rule 1'].str.split(':',expand=True)\n\n# df_train = df_train.drop(columns = ['rule 1'])\n\n# df_train_1.columns = ['Advert','spam']\n\n# df_train_1 = df_train_1.drop(columns = ['Advert'])\n    \n# df_train_1['spam allowed'] = df_train_1['spam'].apply(lambda x: 0 if 'spam' in str(x).lower() else 1)\n\n# df_train['spam allowed'] = df_train_1['spam allowed']\n\n# ### Testing Data \n\n# df_test = df_test_original['rule'].str.split(',', expand=True)\n\n# df_test.fillna('0', inplace=True)\n\n# df_test.columns = ['rule 1', 'rule 2', 'rule 3', 'rule 4']\n\n# ## till here it works perfectly\n\n# df_test['rule 2'] = df_test['rule 2'].apply(lambda x: 1 if 'referral links' in str(x).lower() else 0)\n\n# df_test = df_test.rename(columns={'rule 2': 'referral link allowed'})\n\n# # column 0 (legal_advice)\n# df_test['legal_advice_allowed'] = df_test['rule 1'].apply(lambda x: 0 if 'legal' in str(x).lower() else 1)\n\n# # column 1\n# df_test['rule 4'] = df_test['rule 4'].apply(lambda x: 0 if 'promotional' in str(x).lower() else 1)\n\n# df_test = df_test.rename(columns={'rule 4': 'Promotions allowed'})\n\n# # column 1: advertising\n# df_test['rule 3'] = df_test['rule 3'].apply(lambda x: 0 if 'advertising' in str(x).lower() else 1)\n\n# df_test = df_test.rename(columns={'rule 3': 'Advertising allowed'})\n\n# ## Dealing with spam part using new created dataframe\n# df_test_1 = df_test['rule 1'].str.split(':', expand=True)\n\n# df_test = df_test.drop(columns=['rule 1'])\n\n# df_test_1.columns = ['Advert', 'spam']\n\n# df_test_1 = df_test_1.drop(columns=['Advert'])\n    \n# df_test_1['spam allowed'] = df_test_1['spam'].apply(lambda x: 0 if 'spam' in str(x).lower() else 1)\n\n# df_test['spam allowed'] = df_test_1['spam allowed']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.353916Z","iopub.execute_input":"2025-09-29T10:27:08.354088Z","iopub.status.idle":"2025-09-29T10:27:08.369026Z","shell.execute_reply.started":"2025-09-29T10:27:08.354074Z","shell.execute_reply":"2025-09-29T10:27:08.368313Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# for i in df_train.columns:\n#     df_train_original[i] = df_train[i]\n\n# for i in df_test.columns:\n#     df_test_original[i] = df_test[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.369768Z","iopub.execute_input":"2025-09-29T10:27:08.369944Z","iopub.status.idle":"2025-09-29T10:27:08.387783Z","shell.execute_reply.started":"2025-09-29T10:27:08.369931Z","shell.execute_reply":"2025-09-29T10:27:08.387147Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_original = df_train_original[['body', 'rule']]\ndf_original.loc[:,'rule_violation'] = df_train_original['rule_violation'].values\n    \ndf_p1 = df_train_original[['positive_example_1','rule_violation','rule']]\ndf_p1.loc[:,'rule_violation'] = 0\ndf_p1 = df_p1.rename(columns = {'positive_example_1':'body'})\n        \ndf_p2 = df_train_original[['positive_example_2','rule_violation','rule']]\ndf_p2.loc[:,'rule_violation'] = 0\ndf_p2 = df_p2.rename(columns = {'positive_example_2':'body'})\n        \ndf_n1 = df_train_original[['negative_example_1','rule_violation','rule']]\ndf_n1.loc[:,'rule_violation'] = 1\ndf_n1 = df_n1.rename(columns = {'negative_example_1':'body'})\n        \ndf_n2 = df_train_original[['negative_example_2','rule_violation','rule']]\ndf_n2.loc[:,'rule_violation'] = 1\ndf_n2 = df_n2.rename(columns = {'negative_example_2':'body'})\n    \ndf_train_total = pd.concat([df_original,df_p1,df_p2,df_n1,df_n2], axis = 0,ignore_index=True)\n\n##TESTING DATA\n\ndf_test_total = df_test_original[['body','rule']]        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.388356Z","iopub.execute_input":"2025-09-29T10:27:08.388552Z","iopub.status.idle":"2025-09-29T10:27:08.424136Z","shell.execute_reply.started":"2025-09-29T10:27:08.388538Z","shell.execute_reply":"2025-09-29T10:27:08.423291Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4048906974.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_original.loc[:,'rule_violation'] = df_train_original['rule_violation'].values\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df_train_total.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.424960Z","iopub.execute_input":"2025-09-29T10:27:08.425670Z","iopub.status.idle":"2025-09-29T10:27:08.447693Z","shell.execute_reply.started":"2025-09-29T10:27:08.425649Z","shell.execute_reply":"2025-09-29T10:27:08.447019Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"body              0\nrule              0\nrule_violation    0\ndtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def cleaning_text(text_file):\n    \n     ## Removing URLs\n    pattern = re.compile('https?:\\/\\/\\S+|www\\.\\S+|Https?:\\/\\/\\S+|\\S+\\.com\\S+|\\S+\\.com|\\[.*?\\]|\\S+ \\. com.*')  \n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i])\n        \n    ##Removing HTML rags\n    pattern = re.compile('<.*?>')       \n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i]) \n        \n    ## Removing Emails and Hashtags\n    pattern = re.compile('#\\S+|@\\S+|\\S+\\@\\S+|\\S+@')             \n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i]) \n        \n    ### Removing username and subreddit mentions\n    pattern = re.compile('u\\/\\S+|r\\/\\S+')\n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i])\n        \n    #emotions, symbols, pictographs, transport and map symbols, flags etx.        \n    pattern = re.compile(\"[\"\n                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                            u\"\\U00002702-\\U000027B0\"\n                            u\"\\U000024C2-\\U0001F251\"\n                            \"]+\", flags=re.UNICODE)\n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i])\n        \n    ##Removing Numbers & \\n spaces\n    pattern = re.compile('\\d|\\\\n')             \n    for i in range(len(text_file)):\n        text_file[i] = pattern.sub(r'',text_file[i])\n\n    return text_file\n\n##TRAINING DATA ----------------------\n\ntext_train_file = df_train_total['body'].to_list()\ntext_train_processed_file = cleaning_text(text_train_file)\n\n##TESTING DATA -------------------------------------------------------\n\ntext_testing_file = df_test_total['body'].to_list()\ntext_testing_processed_file = cleaning_text(text_testing_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.448664Z","iopub.execute_input":"2025-09-29T10:27:08.448968Z","iopub.status.idle":"2025-09-29T10:27:08.915136Z","shell.execute_reply.started":"2025-09-29T10:27:08.448940Z","shell.execute_reply":"2025-09-29T10:27:08.914571Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef preprocessing_data(dataframe, text_file):\n    cleaned_text = []\n    preprocessed_text = []\n    for i in range(len(dataframe)):\n        text1 = re.sub('[^a-zA-Z]',' ',text_file[i])\n        text1 = text1.lower()\n        text1 = [lemmatizer.lemmatize(word) for word in text1 if word not in combined_stopwords]\n        cleaned_text = ''.join(text1)\n        preprocessed_text.append(cleaned_text)\n    return preprocessed_text\n\n\npreprocessed_training_text = preprocessing_data(dataframe=df_train_total,text_file =  text_train_processed_file)\npreprocessed_testing_text1 = preprocessing_data(dataframe = df_test_total, text_file = text_testing_processed_file )\n\ntrain_rule_violation = df_train_total['rule'].to_list()\ntest_rule_violation = df_test_total['rule'].to_list()\n\nrule_train = preprocessing_data(dataframe=df_train_total, text_file = train_rule_violation)\nrule_test = preprocessing_data(dataframe=df_test_total, text_file = test_rule_violation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:08.915746Z","iopub.execute_input":"2025-09-29T10:27:08.915941Z","iopub.status.idle":"2025-09-29T10:27:15.964939Z","shell.execute_reply.started":"2025-09-29T10:27:08.915925Z","shell.execute_reply":"2025-09-29T10:27:15.964295Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df_train_total['body'] = preprocessed_training_text\ndf_test_total['body'] = preprocessed_testing_text1  \n\ndf_train_total['rule'] = rule_train\ndf_test_total['rule'] = rule_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:15.965643Z","iopub.execute_input":"2025-09-29T10:27:15.965907Z","iopub.status.idle":"2025-09-29T10:27:15.972886Z","shell.execute_reply.started":"2025-09-29T10:27:15.965885Z","shell.execute_reply":"2025-09-29T10:27:15.971981Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3462694056.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_test_total['body'] = preprocessed_testing_text1\n/tmp/ipykernel_36/3462694056.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_test_total['rule'] = rule_test\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"x = df_train_total.drop(columns = ['rule_violation'])\ny = df_train_total['rule_violation']\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.75, random_state = 42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:15.975427Z","iopub.execute_input":"2025-09-29T10:27:15.975609Z","iopub.status.idle":"2025-09-29T10:27:16.039510Z","shell.execute_reply.started":"2025-09-29T10:27:15.975595Z","shell.execute_reply":"2025-09-29T10:27:16.038854Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"##Now we have to do some tokenization\n\nvector = TfidfVectorizer(max_features = 2500, ngram_range= (1,2))\n\nx_train_tokenized = vector.fit_transform(x_train[['body','rule']])\nx_test_tokenized = vector.transform(x_test[['body','rule']])\n\nxgb = XGBClassifier()\n\nxgb.fit(x_train_tokenized,y_train)\n\ny_pred = xgb.predict_proba(x_test_tokenized)\n\n#print(roc_auc_score(y_test,y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:16.040339Z","iopub.execute_input":"2025-09-29T10:27:16.040632Z","iopub.status.idle":"2025-09-29T10:27:40.912159Z","shell.execute_reply.started":"2025-09-29T10:27:16.040612Z","shell.execute_reply":"2025-09-29T10:27:40.911603Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# #Tried to work with training data only\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n\n\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('body_vec', TfidfVectorizer(max_features=4000, ngram_range=(1,2)), 'body'),\n#         ('rule_vec', TfidfVectorizer(max_features=50, ngram_range=(1,2)), 'rule')\n#     ],\n#     remainder='drop'\n# )\n\n# # Create pipeline with preprocessing + model\n# pipeline = Pipeline([\n#     ('tfidf', preprocessor),\n#     ('clf', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n# ])\n\n# # Fit\n# pipeline.fit(x_train, y_train)\n\n# # Predict\n# y_pred = pipeline.predict_proba(x_test)[:,1]\n\n# print(roc_auc_score(y_test,y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:40.912663Z","iopub.execute_input":"2025-09-29T10:27:40.914324Z","iopub.status.idle":"2025-09-29T10:27:40.917916Z","shell.execute_reply.started":"2025-09-29T10:27:40.914301Z","shell.execute_reply":"2025-09-29T10:27:40.917414Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def data_to_list(df):\n    body_file = df['body'].to_list()\n    rule_file = df['rule'].to_list()\n    return body_file,rule_file\n\nx_train_body, x_train_rule = data_to_list(x_train)\nx_test_body, x_test_rule = data_to_list(x_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:36:15.695136Z","iopub.execute_input":"2025-09-29T10:36:15.695441Z","iopub.status.idle":"2025-09-29T10:36:15.700810Z","shell.execute_reply.started":"2025-09-29T10:36:15.695418Z","shell.execute_reply":"2025-09-29T10:36:15.700092Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def tokenize_to_seq(body_file, max_words, num_word, token=None):\n    if token is None:\n        token = Tokenizer(num_words=max_words, oov_token='<OOV>')\n        token.fit_on_texts(body_file)    \n    max_length = max(len(i.split()) for i in body_file)\n    training_sequence = token.texts_to_sequences(body_file)\n    body_sequenced = pad_sequences(training_sequence, maxlen=num_word, padding='post')\n    return body_sequenced, max_length, token\n\nmax_words = 3000\nnum_words = 100  \n\nx_train_body, x_train_max_length, token = tokenize_to_seq(x_train_body, max_words, num_words)\nx_test_body, _, _ = tokenize_to_seq(x_test_body, max_words, num_words, token)\n\n\ndef rule_to_seq(rule_file, token=None, max_rule_word=100, num_word_rule=100):\n    if token is None:\n        token_rule = Tokenizer(num_words=max_rule_word, oov_token='<OOV>')\n        token_rule.fit_on_texts(rule_file)\n    else:\n        token_rule = token\n    max_length_rule_train = max(len(i.split()) for i in rule_file)\n    rule_training_sequence = token_rule.texts_to_sequences(rule_file)\n    rule_sequenced = pad_sequences(rule_training_sequence, maxlen=num_word_rule, padding='post')\n    return rule_sequenced, max_length_rule_train, token_rule\n\nnum_word_rule = 100\n\nx_train_rule_sequenced, x_train_rule_max_length, token_rule = rule_to_seq(x_train_rule, token=None, max_rule_word=100, num_word_rule=100)\nx_test_rule_sequenced, _, _ = rule_to_seq(x_test_rule, token_rule, max_rule_word=100, num_word_rule=100)\n\n\n# ==========================\n# Model Architecture\n# ==========================\n## Text branch\ntext_input = Input(shape=(num_words,))\nembedding_text = Embedding(input_dim=max_words, output_dim=128)(text_input)\nx1_text = GRU(64, return_sequences=True)(embedding_text)\n\n## Rule branch\nrule_input = Input(shape=(num_word_rule,))\nembedding_rule = Embedding(input_dim=100, output_dim=128)(rule_input)  # max_rule_word = 100\nx1_rule = GRU(64, return_sequences=True)(embedding_rule)\n\n# Merge\ncombined = Concatenate()([x1_text, x1_rule])\npooled = GlobalAveragePooling1D()(combined)\nrnn1 = Dense(64, activation='relu', kernel_regularizer=l2(0.5))(pooled)\noutput = Dense(1, activation='sigmoid')(rnn1)\n\nmodel = Model(inputs=[text_input, rule_input], outputs=output)\nadam = Adam(learning_rate=0.001)\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=[AUC(curve='ROC', name='roc_auc')])\n\nhistory = model.fit(\n    [x_train_body, x_train_rule_sequenced],\n    y_train,\n    validation_data=([x_test_body, x_test_rule_sequenced], y_test),\n    epochs=25, batch_size=64, verbose=1,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:27:41.046452Z","iopub.execute_input":"2025-09-29T10:27:41.046686Z","iopub.status.idle":"2025-09-29T10:28:42.453853Z","shell.execute_reply.started":"2025-09-29T10:27:41.046667Z","shell.execute_reply":"2025-09-29T10:28:42.453288Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1759141662.310065      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759141668.524183     102 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 27.9955 - roc_auc: 0.5480 - val_loss: 4.5828 - val_roc_auc: 0.6515\nEpoch 2/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 2.9029 - roc_auc: 0.7146 - val_loss: 0.7927 - val_roc_auc: 0.7509\nEpoch 3/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.6533 - roc_auc: 0.7811 - val_loss: 0.5766 - val_roc_auc: 0.7902\nEpoch 4/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5350 - roc_auc: 0.7994 - val_loss: 0.5462 - val_roc_auc: 0.8173\nEpoch 5/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5477 - roc_auc: 0.8014 - val_loss: 0.5745 - val_roc_auc: 0.8042\nEpoch 6/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5285 - roc_auc: 0.8166 - val_loss: 0.5482 - val_roc_auc: 0.8141\nEpoch 7/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4830 - roc_auc: 0.8535 - val_loss: 0.5774 - val_roc_auc: 0.7700\nEpoch 8/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5518 - roc_auc: 0.7766 - val_loss: 0.5788 - val_roc_auc: 0.7755\nEpoch 9/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5089 - roc_auc: 0.8117 - val_loss: 0.5709 - val_roc_auc: 0.7917\nEpoch 10/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.5056 - roc_auc: 0.8158 - val_loss: 0.5503 - val_roc_auc: 0.7956\nEpoch 11/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4943 - roc_auc: 0.8259 - val_loss: 0.5236 - val_roc_auc: 0.8224\nEpoch 12/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4800 - roc_auc: 0.8518 - val_loss: 0.5344 - val_roc_auc: 0.8234\nEpoch 13/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4647 - roc_auc: 0.8660 - val_loss: 0.5091 - val_roc_auc: 0.8329\nEpoch 14/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4653 - roc_auc: 0.8601 - val_loss: 0.5138 - val_roc_auc: 0.8309\nEpoch 15/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4211 - roc_auc: 0.8822 - val_loss: 0.5063 - val_roc_auc: 0.8341\nEpoch 16/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4152 - roc_auc: 0.8880 - val_loss: 0.5113 - val_roc_auc: 0.8325\nEpoch 17/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4245 - roc_auc: 0.8822 - val_loss: 0.5109 - val_roc_auc: 0.8340\nEpoch 18/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4056 - roc_auc: 0.8927 - val_loss: 0.5376 - val_roc_auc: 0.8328\nEpoch 19/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4087 - roc_auc: 0.8914 - val_loss: 0.5103 - val_roc_auc: 0.8332\nEpoch 20/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4081 - roc_auc: 0.8914 - val_loss: 0.4962 - val_roc_auc: 0.8323\nEpoch 21/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.3935 - roc_auc: 0.8954 - val_loss: 0.5238 - val_roc_auc: 0.8343\nEpoch 22/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4089 - roc_auc: 0.8938 - val_loss: 0.4937 - val_roc_auc: 0.8328\nEpoch 23/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.3942 - roc_auc: 0.8955 - val_loss: 0.4990 - val_roc_auc: 0.8202\nEpoch 24/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4006 - roc_auc: 0.8937 - val_loss: 0.5117 - val_roc_auc: 0.8308\nEpoch 25/25\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.4044 - roc_auc: 0.8906 - val_loss: 0.4966 - val_roc_auc: 0.8335\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"new_body, new_rule = data_to_list(df_test_total)\n\nx_new_body, _, _ = tokenize_to_seq(new_body, max_words, num_words, token=token)\nx_new_rule, _, _ = rule_to_seq(new_rule, token_rule, max_rule_word=100, num_word_rule=100)\n\n# Predict probabilities\ny_new_proba = model.predict([x_new_body, x_new_rule])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:37:04.304141Z","iopub.execute_input":"2025-09-29T10:37:04.304826Z","iopub.status.idle":"2025-09-29T10:37:04.399134Z","shell.execute_reply.started":"2025-09-29T10:37:04.304802Z","shell.execute_reply":"2025-09-29T10:37:04.398416Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Predict\nsubmission = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv')\nsubmission['rule_violation'] = np.round(y_new_proba,2)\nsubmission.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"rule_violation\"])\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T10:37:14.131750Z","iopub.execute_input":"2025-09-29T10:37:14.132054Z","iopub.status.idle":"2025-09-29T10:37:14.143845Z","shell.execute_reply.started":"2025-09-29T10:37:14.132032Z","shell.execute_reply":"2025-09-29T10:37:14.143138Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"   row_id  rule_violation\n0    2029            0.81\n1    2030            0.33\n2    2031            0.17\n3    2032            0.64\n4    2033            0.33\n5    2034            0.77\n6    2035            0.84\n7    2036            0.54\n8    2037            0.82\n9    2038            0.23","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>rule_violation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2029</td>\n      <td>0.81</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2030</td>\n      <td>0.33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2031</td>\n      <td>0.17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2032</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2033</td>\n      <td>0.33</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2034</td>\n      <td>0.77</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2035</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2036</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2037</td>\n      <td>0.82</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2038</td>\n      <td>0.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}